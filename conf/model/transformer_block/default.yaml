_target_: pprl.models.modules.transformer.TransformerBlock
_partial_: True  # will get called several times
attention:
  _target_: torch.nn.MultiheadAttention
  _partial_: True  # requires `embed_dim`
  num_heads: 6
  dropout: 0.0  # this is the default
  bias: True  # this is the default
  add_bias_kv: False  # this is the default
  batch_first: True
mlp:
  _target_: pprl.models.modules.mlp.TransformerBlockMLP
  _partial_: True  # requires `embed_dim`
  mlp_ratio: 3
  act: gelu
  norm: null
  dropout: 0.0  # this is the default
